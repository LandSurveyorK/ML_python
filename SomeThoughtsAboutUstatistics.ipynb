{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SomeThoughtsAboutUstatistics.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/LandSurveyorK/ML_python/blob/master/SomeThoughtsAboutUstatistics.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "l9hQg3DfNAjw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## U statistics, Bagging, and Random Forest"
      ]
    },
    {
      "metadata": {
        "id": "S_O0fz54NRlh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###  Bagging introduces exta randomness $Z\\sim \\text{Bern}(p)$. \n",
        "$$U= {n \\choose k}^{-1}\\sum z h(x_{i1},\\cdots, x_{ik})$$\n",
        "\n",
        "then \n",
        "\n",
        "$$ \\zeta_1 \\to  p^2\\zeta_1, \\zeta_k \\to p \\zeta_k=\\zeta_k^* $$\n",
        "since $\\zeta^*_1 = cov(zh(x_1,x_2,\\cdots, x_k), z'h(x_1,x_2',\\cdots,x_n')) = p^2\\zeta_1$, and $\\zeta_k^* = cov(zh(x_1,\\cdots, x_k), zh(x_1,\\cdots, x_k)) = p\\zeta_k$.\n",
        "\n",
        "### H-decompostion\n",
        "- $h^{(1)} \\to  E[Zh|X_1] = ph^{(1)}$\n",
        "- $h^{(2)} \\to  E[Zh|X_1ï¼ŒX_2] - E[Zh|X_1]- E[Zh|X_2] = ph^{(2)}$\n",
        "- $\\cdots$\n",
        "- $h^{(k)} \\to  Zh - p\\sum_{j=1}^{k-1}{k \\choose j}h^{(j)}(X_{i1}, \\cdots, X_{ij})= Zh -p\\sum_{j=1}^{k-1}{k \\choose j}h^{(j)}(X_{i1},\\cdots, X_{ij})$"
      ]
    },
    {
      "metadata": {
        "id": "37jm6I7dQhgA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are still uncorrelated, but in $h^{(k)}$, we use $Zh$ instead of $h$. Furthermore \n",
        "\n",
        "$$V_j \\to p^2V_j = V_j^*, \\quad j = 1\\cdots, k-1$$\n",
        "$$V_k \\to p\\sum_{j=1}^k{k \\choose j}V_j - p^2\\sum_{j=1}^{k-1}{k \\choose j }V_j = V_k^* = p^2V_k+p(1-p)\\zeta_k$$\n",
        "It can also be showed by law of total varaince. \n",
        "\n",
        "Then\n",
        "$U = \\sum_{j=1}^k {k\\choose j}H^{(j)}$, where $H^{(j)} = {n \\choose j}^{-1}\\sum h^{(j)(X_{i1}, \\cdots X_{ij})}$.\n",
        "\n",
        "$$Var(U) = \\sum {k \\choose j}^2{n\\choose j}^{-1}V_j^*$$\n",
        "$$Var(h) = \\sum {k \\choose j}V_j^*$$\n",
        "\n",
        "$Var(U) - \\frac{k^2}{n}V^*_1 \\leq \\frac{k^2}{n^2}{ Var(h)}$\n",
        "$$\\frac{var(U-\\hat{U})}{Var(\\hat{U})} \\leq \\frac{k}{n}\\frac{\\zeta^*_k}{k\\zeta^*_1}$$\n",
        "but $\\zeta_k^*/k\\zeta^*_1 \\to \\frac{1}{p} \\zeta_k/k\\zeta_1 = \\frac{{n \\choose k}}{N}\\frac{\\zeta_k}{k\\zeta_1}$, \n",
        "$$\\frac{k}{n}\\times \\frac{{n \\choose k}}{N}$$\n",
        "\n",
        "U statistics itself might not be approximated within first order, bagging is even worse? \n",
        "$$\\hat{U} \\to p \\frac{k}{n}\\sum E[h|X_i] ,\\quad \\frac{\\sqrt{n}U}{\\sqrt{p^2k^2\\zeta_1}} = \\frac{U_N}{\\sqrt{\\frac{k^2}{n}\\zeta_1}}$$\n",
        "Absolutely,  that's why in the theorem I established, \n",
        "$$ \\frac{U_N }{\\sqrt{\\frac{k^2}{n}\\zeta_1} + \\frac{1}{N}\\zeta_k}$$\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "s6ImPLgUYnu2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### How variance decreases with bagging?\n",
        "- $$\n",
        "\\begin{aligned}c\\zeta^*_d - d\\zeta^*_c &= c\\sum_{j=1}^d{d \\choose j}V^*_j - d\\sum_{j=1}^c{c \\choose j}V^*_j\\\\\n",
        "&\\geq \\sum_{j=1}^c \\left(c{d \\choose j}-d{c\\choose j}\\right)V^*_j\\\\\n",
        "&\\geq 0\n",
        "\\end{aligned}$$\n",
        "\n",
        "<!--- $$\n",
        "\\begin{aligned}\n",
        "Var(U) &= {n \\choose k}^{-1}\\sum_{c=1}^{k}{k \\choose c}{n-c\\choose k-c} \\zeta^*_c \\\\\n",
        " & \\leq {n \\choose k}^{-1}\\sum_{c=1}^{k}{k \\choose c}{n-c\\choose k-c} \\frac{c}{k}\\zeta^*_k \\\\\n",
        " & = \\frac{k}{n}\\zeta^*_k \\to  \\frac{k}{n}p\\zeta_k  \n",
        "\n",
        "\\end{aligned}$$\n",
        "-->\n",
        "\n",
        "- Simple verification \n",
        "$$\n",
        "\\begin{aligned}\n",
        "Var({n \\choose k}^{-1}\\sum zh ) &= E[Var(U'|X_1,\\cdots, X_n)] + Var(E[U'|X_1,\\cdots, X_n])\\\\\n",
        "& = E[p_n(1-p){n \\choose k}^{-1}U(h^2)] + Var(pU(h)) \\\\\n",
        "& = p(1-p){n \\choose k}^{-1}E(U(h^2)) + p^2Var(U) \\\\\n",
        "& = p(1-p){n \\choose k}^{-1}\\zeta_k + p^2 Var(U)\\\\\n",
        "\\end{aligned}$$\n",
        "Therefore \n",
        "\n",
        "$$Var(U_N) = Var(U) + (1-p)\\frac{1}{N}\\zeta_k = Var(U)+\\left(\\frac{1}{N}-{n \\choose k}^{-1}\\right)\\zeta_k$$"
      ]
    },
    {
      "metadata": {
        "id": "-6v519nuh2Hd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Higher order Approximation without using complete U statistics\n",
        "\n",
        "- Guess \n",
        "$$\\frac{U_N}{\\sqrt{\\frac{\\zeta_k}{N} + *}}$$\n",
        "- $\\zeta_k \\geq \\frac{1}{p}\\times (k\\zeta_1)$, if $N = O(n)$, then comparing to $\\zeta_k$, all other terms are negligible. \n",
        "\n",
        " $$\n",
        "\\begin{aligned}\n",
        "Var(U) &= {n \\choose k}^{-1}\\sum_{c=1}^{k-1}{k \\choose c}{n-c\\choose k-c} p^2\\zeta_c^*  + {n \\choose k}^{-1} \\zeta^*_k\\\\\n",
        " & \\leq {n \\choose k}^{-1}\\sum_{c=1}^{k-1}{k \\choose c}{n-c\\choose k-c} \\frac{c}{k}p^2 \\zeta_k +  {n \\choose k}^{-1} \\zeta^*_k  \\\\\n",
        " & = p^2 \\frac{k}{n}\\zeta_k  + {n \\choose k}^{-1}( p-p^2)\\zeta_k \\\\\n",
        " & = p^2(\\frac{k}{n}- {n \\choose k}^{-1} + \\frac{1}{N})\\zeta_k\n",
        "\\end{aligned}$$\n",
        "In other words, \n",
        "\n",
        "$$Var(U_N) \\sim \\frac{1}{N}\\zeta_k +\\left(\\frac{k}{n}+ {n \\choose k}^{-1}\\right)\\zeta_k$$\n",
        "\n",
        " $$\n",
        "\\begin{aligned}\n",
        "Var(U) &= {n \\choose k}^{-1}\\sum_{c=1}^{k-1}{k \\choose c}{n-c\\choose k-c} p^2\\zeta_c^*  + {n \\choose k}^{-1} \\zeta^*_k\\\\\n",
        " & \\geq {n \\choose k}^{-1}\\sum_{c=1}^{k-1}{k \\choose c}{n-c\\choose k-c} c p^2 \\zeta_1 +  {n \\choose k}^{-1} \\zeta^*_k  \\\\\n",
        " & = p^2 \\frac{k^2}{n}\\zeta_1  + {n \\choose k}^{-1}( p-p^2)\\zeta_k \\\\\n",
        " & = p^2(\\frac{k^2}{n}\\zeta_1 + ( -{n \\choose k}^{-1} + \\frac{1}{N})\\zeta_k)\n",
        "\\end{aligned}$$\n",
        "\n",
        "\n",
        "$$Var(U_N) \\sim \\frac{1}{N}\\zeta_k +\\frac{k^2}{n}\\zeta_1-  {n \\choose k}^{-1}\\zeta_k$$"
      ]
    },
    {
      "metadata": {
        "id": "-HFfalNBe1sE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In summary, the firt $k-1$ terms is \n",
        "$$\\frac{k^2}{n}\\zeta_1 - {n \\choose k}^{-1}\\zeta_k \\leq \\Delta \\leq \\frac{k}{n}\\zeta_k - {n \\choose k}^{-1}\\zeta_k$$\n",
        "The last term has oder of \n",
        "$$\\frac{1}{N}\\zeta_k$$"
      ]
    },
    {
      "metadata": {
        "id": "YZzepimf8NN4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Consider the variance of $U_N$, even if $\\zeta_k/k\\zeta_1\\leq M$ is bouned, \n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "Var(U_N) &\\geq \\frac{k^2}{n}\\zeta_1 + \\left(\\frac{1}{N}-{n \\choose k}^{-1}\\right)\\zeta_k \n",
        "\\end{aligned}$$\n",
        "\n",
        "\n",
        "$$Var(U_N) \\leq \\frac{k}{n}\\zeta_k + \\left(\\frac{1}{N}-{n \\choose k}^{-1}\\right)\\zeta_k $$\n",
        "\n",
        "- Can $k = O(n)$?\n",
        "Then $k\\zeta_1$ must tend to zero. \n",
        "\n",
        "$Var(U) = \\frac{k^2}{n}V_1 + \\sum_{j=2}^k{k \\choose j}V_j$"
      ]
    },
    {
      "metadata": {
        "id": "qCco6ECFAYNX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### $U_N$'s decompostion\n",
        "\n",
        "$\n",
        "\\begin{aligned}\n",
        "U & = \\sum_{j=1}^{k-1}{k \\choose j}H_n^{(j)} + H_n^{k} \\\\\n",
        "& = \\sum_{j=1}^{k-1}{k \\choose j}pH_n^{(j)} + {n \\choose k}^{-1}\\sum h^{(k)}(X_{i1},\\cdots, X_{ik}) \\\\\n",
        "& = p H_n^{(1)} + p\\sum_{j=2}^{k-1}{k \\choose j}H_n^{(j)} +  {n \\choose k}^{-1}\\sum h^{(k)}(X_{i1},\\cdots, X_{ik}) \n",
        "\\end{aligned}$\n",
        "\n",
        "$$U_N = H_n^{(1)} + \\sum_{j=2}^{k-1}{k \\choose j}H_n^{(j)} + \\frac{1}{N}\\sum h^{(k)}(X_{i1},\\cdots, X_{ik }) = H_n^{(1)}+ \\Delta$$\n",
        "\n",
        "$\\Delta - \\Delta_i = \\sum_{j=2}^{k-1}{k \\choose j}{n \\choose j}^{-1}\\sum_{i \\in }h^{(j)}  + \\frac{1}{N}\\sum_{i \\in}h^{(k)}$\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "E \\Delta^2 & = \\sum_{j=2}^{k-1}{k \\choose j}^2{n \\choose j}^{-1}V_j +\\frac{1}{N^2}{n \\choose k}^{}V_k^* \\\\\n",
        "&  = \\sum_{j=2}^k{k \\choose j}^2{n \\choose j}^{-1}V_j + \\frac{1}{N}(1-p)\\zeta_k\n",
        "\\end{aligned}$$\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "E|\\Delta-\\Delta_i|^2 &= \\sum_{j=2}^k{k\\choose j}^2{n \\choose j}^{-1}\\frac{j}{n}V_j + \\frac{1}{N^2}{n-1 \\choose k-1}V_k^* \\\\\n",
        "& = \\sum_{j=2}^k{k \\choose j}^2{n \\choose j}^{-1}\\frac{j}{n}V_j +\\frac{k}{n} \\frac{1}{N}(1-p)\\zeta_k \n",
        "\\end{aligned}$$\n",
        "\n",
        "If we still use first order to approximate, then \n",
        "$$\\frac{n}{k^2\\zeta_1}E|\\Delta^2|\\leq \\frac{k}{n}\\frac{\\zeta_k}{k\\zeta_1} + \\frac{n}{Nk}(1-p)\\frac{\\zeta_k}{k\\zeta_1}$$\n",
        "\n",
        "$$\\sum\\limits_{i=1}^n  \\frac{n}{k^2\\zeta_1}E|\\Delta_i^2|\\leq \\frac{2k}{n}\\frac{\\zeta_k}{k\\zeta_1} + \\frac{n}{N}(1-p)\\frac{\\zeta_k}{k\\zeta_1}$$\n",
        "\n",
        "Note: \n",
        "- $\\frac{k^2}{n^2}\\sum_{j=2}^k {k \\choose j}V_j\\leq \\frac{k^2}{n^2}\\zeta_k- \\frac{k^3}{n^2}\\zeta_1= \\frac{k}{n}\\frac{\\zeta_k-k\\zeta_1}{k\\zeta_1}$.\n"
      ]
    },
    {
      "metadata": {
        "id": "FA3EISgdW1z-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I believe, the most interesting part of bagging is when $\\zeta_k\\gg  k\\zeta_1$, for example when you introduce extra randomness in building a tree. So first order approximation should not be appropriate. \n",
        "\n",
        "- Randomization helps, higher order approximation \n",
        "- Bagging helps.\n",
        "\n",
        "### JUST Randomization, how it helps?\n",
        "\n",
        "$$\\zeta_1: Var(E[h|X_1])\\to Var(E[E_\\theta h(X,\\theta)|X_1]) $$\n",
        "$$\\zeta_k: Var(h(X))\\to Var(h(X,\\theta))$$\n",
        "So it is possible that $\\zeta_k \\gg k\\zeta_1$. By introducing this extra randomization, the covariance of two trees should decrease, but the variance of a single tree should increase. Then it is more propriate to use the highest order term to approximate $U$, instead of firest order (Wager did not realize the essence). \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "x_k-wL6XTzMy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Getting ride of $\\left(\\frac{m}{n}\\right)^{1/3}$ \n",
        "\n",
        "We used the most loose inequality- Chebyshev's inequality. \n",
        "$$ P(|X-\\mu|\\geq \\delta) \\leq \\frac{E|X-\\mu|^2}{\\delta^2}$$\n",
        "\n",
        "- $P(|\\bar{X}-\\mu|\\geq \\delta) \\leq \\frac{\\sigma^2}{n \\delta^2}$, then the trade of between $\\delta$ and $\\sigma^2/(n\\delta^2)$, makes $\\delta = O(n^{1/3})$.\n",
        "\n",
        "- What if we have exponential or sub exponential tail? If $X_i$ is bounded,  Note Hoeffding\n",
        "\n",
        "$$ P(|\\bar{X}-\\mu|\\geq \\delta) \\leq 2\\exp(-2n\\delta^2/B^2)$$\n",
        "Than the trade off $2\\exp(-2n\\delta^2/B^2)\\sim \\delta \\Rightarrow \\delta \\approx n^{-1/2}$.\n",
        "\n",
        "- U statistics, $Var(U)\\leq \\left(\\frac{m}{n}\\right)^{1/2}\\sigma^2$\n",
        "\n",
        "$$ P\\left[|U- m_h|\\geq ||h||_{\\infty} \\sqrt{\\frac{\\log(\\frac{2}{\\delta})}{2\\lfloor n/m\\rfloor}}\\right]\\leq \\delta$$\n",
        "for $h$ bounded, for all $\\delta>0$ and one can derive \n",
        "$$ P\\left[ |U-m_h|>\\sqrt{\\frac{2\\sigma^2\\log(\\frac{2}{\\delta})}{\\lfloor n/m\\rfloor}}\\vee \\frac{2||h||_{\\infty}\\log(\\frac{2}{\\delta})}{3\\lfloor n/m\\rfloor}\\right]\\leq \\delta $$\n",
        "\n",
        "This two inequalities is not what we want, since we need margin or error devides mean. We want the margin of error involves the standard deviation of the kernel function.\n",
        "\n",
        "-  Reconsider the Cramer- Chernoff Method\n",
        "$$P(Z\\geq t)\\leq \\exp(-\\lambda t)Ee^{\\lambda z}$$\n",
        "    * Standard Normal\n",
        "    $$P(Z\\geq t)\\leq \\exp(-\\lambda t -\\frac{\\lambda^2 \\sigma^2}{2})\\leq\\exp(-t^2/2\\sigma^2) $$\n",
        "    * U statistics: Exponential decade? $h$ is sub-Gaussian\n",
        "     $$\\exp(-\\lambda t)E[e^{\\lambda T_i}]\\leq \\exp(-t^2/2\\sigma^2)$$\n",
        "     \n",
        "-     U-statistics can be written as sums of independent random variables, let $k = \\lfloor n/m\\rfloor$ and \n",
        "    $$V(X_1,\\cdots, X_n) = \\frac{1}{k}\\{h(X_1,\\cdots, X_m)+\\cdots + h(X_{km-m+1}, \\cdots, X_{km})\\}$$\n",
        "One cen see that \n",
        "$$ U = \\frac{\\sum_{\\sigma \\in S_n}V(X_{\\sigma_1, \\cdots , \\sigma_n})}{n!}$$\n",
        "    Unfortunately $U$ itself is not sum average of independent variables, so we might have a larger upper bound (loss $n$).  \n",
        "\n",
        "    Suppose $T$ is a random variable that can be written as \n",
        "$$T = \\sum_{i=1}^N p_iT_i, \\quad \\sum p_i = 1, p_i\\geq 0$$\n",
        "then  $P(T>t)\\leq e^{-st}\\sum \\limits_{i=1}^Np_iE[e^{sT_i}] = \\sum \\limits_{i=1}^N p_ie^{-st}E[e^{sT_i}]$. \n",
        "It means by taking average, things is not worse than individual.  \n",
        "Then we can let $t = \\left(\\frac{m}{n}\\right)^{1/2}\\sigma$. \n",
        "\n",
        "\n",
        "### $L^2$ consistency\n",
        "\n",
        "- Scornet's result $t\\log^9(n)/n\\to 0$ is not useful in practice.\n",
        "- Does not take advantage of random forest, consider $E_{\\theta}[\\text{Tree}(X,\\theta)]$;\n",
        "- $ Var(E_{\\theta}\\text{Tree}(X,\\theta)) = E[E^2_{\\theta}\\text{Tree}(X,\\theta)]\\leq E[E_{\\theta}[\\text{Tree}^2(X,\\theta)]] = Var(T(X,\\theta))$\n",
        "-  $\\text{Bias}^2 = (E[\\text{Tree}(X,\\theta)] - f(x^*))^2 $\n",
        "-  A generic random variable $\\theta$ and\n",
        "independent of data. In practice, this variable is used to resample the training set prior to the growing of individual trees and to select the successive candidate directions for splitting.   \n",
        "-  $E_{\\theta}\\text{Tree}(X,\\theta)$ is very stable, just like a infinite random forest. But Biau and Scornet sill tried to bound $Var(T(X,\\theta))$, which is not necessary. Because instead of considering random forest, they come back to single decision tree. \n",
        "\n",
        "\n",
        "### Close look of Bagging\n",
        "\n",
        "$$U_N = kH_n^{(1)} + \\sum_{j=2}^{k-1}{k \\choose j}H_n^{(j)} + \\frac{1}{N}\\sum h^{(k)}(X_{i1},\\cdots, X_{ik }) = kH_1^{(1)} + H_n^{(k)}+ \\Delta$$\n",
        "\n",
        "- $h^{(k)}$ are not independent.\n",
        "- $Var(U_N)$ is mainly depends on the last term $ \\frac{1}{N}\\sum h^{(k)}(X_{i1},\\cdots, X_{ik })$.\n",
        "- All the terms are uncorrelated. \n",
        "- Central limit theorem for weak dependence. It is a new theorem that need to be developed. \n",
        "\n",
        "\n",
        "### Slutsky theorem\n",
        "\n",
        "- If $\\zeta_k/k\\zeta_1$ bounded and $n = o(N)$, first order approximation is still appropriate, when higher order approximation is appropriate? \n",
        "\n",
        "- $H_n^{k} \\to N(0, {n \\choose k}^{-1}V_k + \\frac{1}{N}(1-p)\\zeta_k) \\sim N(0,\\frac{1}{N}\\zeta_k)$, how to verify? \n",
        "\n",
        "- $Var(k H_n^{(1)} + \\Delta) = \\sum_{j=1}^{k-1}{k \\choose j}^2{n \\choose j}^{-1}V_j$\n",
        "- Sufficient but not necessary condition: $Var(H_n^{(1)}+ \\Delta) \\leq \\frac{k}{n}\\sum_{j=1}^{k}V_j = \\frac{k}{n}\\zeta_k$. Therefore as long as $Nk\\ll n$, slutsky theorem can be applied. It is not really insightful, since then we can choose disjoint blocks.  Contrary to first order approximation, we need $N$ be relatively small. \n",
        "\n",
        "- People want $N = O(n)$, since variance can be controlled, hence, we need a tighter bound for $Var(k H_n^{(1)}+\\Delta)$\n",
        "\n",
        "\n",
        "-  U-statistics is UMVUE for $\\mathcal{F}$, but for a small probability family, it might not has the minimum variance.  But the variance of U statistics is smaller than disjoined block average. Becase disjoined block average is also a ubiased estimator for $\\mathcal{F}$.\n",
        "\n",
        "$$\\sup_{z}\\left|P(\\frac{U}{\\sqrt{k \\zeta_k/n }}\\leq z)- \\phi(z)\\right|\\leq \\frac{c E|h|^3}{(E|h^2|)^{3/2}}\\sqrt{\\frac{k}{n}}$$\n",
        "- For fixed order, the Berry-Essen Bound rate of U statistics and Disjoined Blocks are both $n^{-1/2}$, therefore U statistics helps to reduce the variance, but the diference between normal does not get improved. \n",
        "<!-- $$\\frac{E|g|^2}{n^{-1/2}(E|g|)^{3/2}} + \\sqrt{\\frac{k}{n}\\cdot \\frac{\\zeta_k}{k \\zeta_1}}$$ -->\n",
        "\n",
        "- $$\\frac{k^2}{n}\\zeta_1\\leq Var(U)\\leq \\frac{k^2}{n}\\zeta_1  + \\frac{k^2}{n^2}\\zeta_k $$\n",
        "$$\\frac{\\zeta_k}{k\\zeta_1 + \\frac{k}{n}\\zeta_k}\\leq Var(B)/Var(U) \\leq \\frac{k/n\\zeta_k}{Var(U)}\\leq\\frac{\\zeta_k}{k\\zeta_1}$$\n",
        "Wager make s the assumtion: $\\frac{k}{n}\\zeta_k \\ll k\\zeta_1$. We can still benefit from $U$ comparing to $B$.\n",
        "\n",
        "- Then why we choose $U$ instead of $B$ ? the bound of the second or higher order terms of the variance of U statistics should be much smaller than what we have so far. Then we can benefit more from $U$.\n",
        "\n",
        "- Suppose the varianace of $U$ is much less than $B$, why? \n",
        " $$Var(U) = \\cdots  + {n \\choose k}^{-1}\\zeta_k, \\quad Var(B) = \\frac{k}{n}\\zeta_k$$\n",
        " We benifits from $U$ because of high variance kernel, Incomplete U statistics takes computation cost into account, thus the variance does not reduce that much. \n",
        "\n",
        "- Although Incomplete U statistics does not reduce that much variance as U statistics, under the assumtion $\\frac{k}{n}\\frac{\\zeta_k}{k\\zeta_1}$ and $n = o(N)$, the variance of incomplete U statisitcs is much smalle than disjoined blocks $B$, and has a asymptotic normality. $\\frac{k}{n}\\frac{\\zeta_k}{k\\zeta_1}$ should be replaced with other conditions, as long as we have the asymptotic normality of U statisitcs, or asymptotic normality of incomplete U statistics.\n",
        "\n",
        "- **Contradiction**: To reduce more variance, we make $\\zeta_k$ larger relatively to covariance; To approximate U statisitcs with first order, we want to control $\\zeta_k$ not too large. \n",
        "\n",
        "- $Var(\\Delta)$ is much smaller than the bound we derived? so the first order term stil.l dominates? Or first oder approximation is no longer appropriate. \n",
        "\n",
        "### $Var(\\Delta)$\n",
        "\n",
        "- Much smaller than $\\frac{k^2}{n}\\zeta_1$, U statistics is close to normal. (It seems like it is hard to give a tighter bound of $Var(\\Delta)$)\n",
        "\n",
        "\n",
        "- No longer want the first term dominate, Is U statistics still asypmtotically normal? what's the varaince, and what's the Berry-essen bound? \n",
        "\n",
        "\n",
        "##  U statistics\n",
        "\n",
        "- $U = \\sum_{j=1}^{k-1}{k \\choose j}H_n^{(j)} + H_n^{(k)} = \\sum_{j=1}^{k-1}{k \\choose j}H_n^{(j)} +  {n \\choose k}^{-1}\\sum h^{(k)}(X_{i1},\\cdots , X_{ik})$\n",
        "\n",
        "- $V_k= \\sum_{j=1}^k(-1)^{k-j}{k \\choose j}\\zeta_j = \\zeta_k + \\sum_{j=1}^{k-1}(-1)^{k-j}{k \\choose j}\\zeta_j $\n",
        "- The last term dominates\n",
        "- The last term is also a U statistics, but all covariance term $\\zeta_1',\\cdots,\\zeta_{k-1}'=0'$, $\\zeta_k'=V_k$.\n",
        "- \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- Variance  \n",
        "$$Var(U_N) = Var(U)+ \\left(\\frac{1}{N}- {n \\choose k}^{-1}\\right)\\zeta_k\\geq \\frac{k^2\\zeta_1}{n} +  \\left(\\frac{1}{N}- {n \\choose k}^{-1}\\right)\\zeta_k$$\n",
        "\n",
        "\n",
        "\n",
        "<!--  $\\zeta_k - 2^k\\zeta_{k-1}\\leq V_k\\leq \\zeta_k + 2^k\\zeta_{k-1}$ -->\n",
        "\n",
        "- To make variance of $U_N$ be smaller than $B$, then $N \\geq \\frac{n}{k}$.\n"
      ]
    },
    {
      "metadata": {
        "id": "I8vcdvA4kYsY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### U statistics and Disjoined Blocks \n",
        "- Disjoined blocks, the variance is $\\frac{k}{n}\\zeta_k$.\n",
        "- U statistics \n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "Var(U) &= {n \\choose k}^{-1}\\sum_{c=1}^{k}{k \\choose c}{n-c\\choose k-c} \\zeta_c \\\\\n",
        " & \\leq {n \\choose k}^{-1}\\sum_{c=1}^{k}{k \\choose c}{n-c\\choose k-c} \\frac{c}{k} \\zeta_k +   \\\\\n",
        " & =  \\frac{k}{n}\\zeta_k   \\\\\n",
        "\\end{aligned}$$\n",
        "\n",
        "We know that $\\zeta_c = \\sum_{j=1}^c{k \\choose j}V_j$, \n",
        "$$c\\zeta_d - d\\zeta_c = \\sum_{j=1}^c\\left(c{d \\choose j}- d{c \\choose j}\\right)V_j + \\sum_{j=c+1}^dc{d \\choose j}V_j$$\n",
        "Therefore,\n",
        "\n",
        "$$\\zeta_c =  \\frac{c}{d}\\zeta_d - \\frac{c}{d}\\sum_{j=c+1}^d {d \\choose j}V_j - \\sum_{j=2}^c\\left(\\frac{c}{d}{d \\choose j}- {c\\choose j}\\right)V_j$$\n",
        "\n",
        "Only when $V_j= 0$, $\\forall j =2,\\cdots, k$, $\\zeta_c = \\frac{c}{d}\\zeta_d$.\n",
        "\n",
        "- $\\zeta_k = \\sum_{j=1}^k {k \\choose j}V_j=  k\\zeta_1 +  \\sum_{j=2}^k{k \\choose j}V_j$.\n",
        "\n",
        "- Still hard to quantify how much varaince is been reduced by U statistics comparing to Disjoined blocks\n",
        "\n",
        "\n",
        "\n",
        "###  Stein's Method\n"
      ]
    }
  ]
}